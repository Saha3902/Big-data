{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsR72X23bok33YFfpAgwkg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saha3902/Big-data/blob/main/big_data_tp02_%D9%85%D8%B9%D8%AF%D9%84\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0DYn6flBocY",
        "outputId": "cb7cb661-86d4-4763-b37e-44e228fc96f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "=== 1ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Kaggle API Token ===\n",
            "âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù kaggle.json Ø§Ù„Ù…Ø±ÙÙˆØ¹ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ.\n",
            "\n",
            "âœ… Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§: /content/large_5gb_sample.csv (0.10 GB)\n",
            "\n",
            "=== 3ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: pandas.read_csv(chunksize) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ù€ Pandas:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:00<00:00,  5.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Ø§Ù„Ù…ØªÙˆØ³Ø· (Pandas Chunk): 49.9919\n",
            "â±ï¸ Ø§Ù„Ø²Ù…Ù†: 0.91 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "=== 4ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Dask DataFrame ===\n",
            "ğŸ“Š Ø§Ù„Ù…ØªÙˆØ³Ø· (Dask): 49.9919\n",
            "â±ï¸ Ø§Ù„Ø²Ù…Ù†: 2.03 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "=== 5ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet: 3it [00:01,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 3 Ù…Ù„Ù Parquet\n",
            "ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø¬Ù… (Parquet): 40.82 MB\n",
            "â±ï¸ Ø²Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„ (CSV -> Parquet): 1.78 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "ğŸ”¹ Ù‚Ø±Ø§Ø¡Ø© ÙˆØ­Ø³Ø§Ø¨ Ù…Ù† Parquet (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask)...\n",
            "ğŸ“Š Ø§Ù„Ù…ØªÙˆØ³Ø· (Dask Parquet): 49.9919\n",
            "â±ï¸ Ø²Ù…Ù† Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„Ø­Ø³Ø§Ø¨: 0.12 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "=== 6ï¸âƒ£ ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ===\n",
            "Ø­Ø¬Ù… CSV Ø§Ù„Ø£ØµÙ„ÙŠ: 98.04 MB\n",
            "----------------------------------------\n",
            "Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1 (Pandas Chunk): | Ø§Ù„Ù…ØªÙˆØ³Ø· 49.9919 | Ø§Ù„Ø²Ù…Ù† 0.91s | Ø§Ù„ØªØ®Ø²ÙŠÙ†: 98.04 MB\n",
            "Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2 (Dask CSV): Â  Â  | Ø§Ù„Ù…ØªÙˆØ³Ø· 49.9919 | Ø§Ù„Ø²Ù…Ù† 2.03s | Ø§Ù„ØªØ®Ø²ÙŠÙ†: 98.04 MB\n",
            "Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3 (Dask Parquet): | Ø§Ù„Ù…ØªÙˆØ³Ø· 49.9919 | Ø§Ù„Ø²Ù…Ù† (Ù‚Ø±Ø§Ø¡Ø©) 0.12s | Ø§Ù„ØªØ®Ø²ÙŠÙ†: 40.82 MB (Ù…Ø¶ØºÙˆØ·)\n",
            "Ø²Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ: Â  Â  Â  Â  Â | - Â  Â  Â  Â  Â  Â  Â  Â  Â | Ø§Ù„Ø²Ù…Ù† (ØªØ­ÙˆÙŠÙ„) 1.78s\n",
            "\n",
            "âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù†Ø¬Ø§Ø­.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# TP02: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª CSV Ø¶Ø®Ù…Ø© (Ø­ÙˆØ§Ù„ÙŠ 5GB) ÙÙŠ Google Colab\n",
        "# ============================================================\n",
        "\n",
        "# âš™ï¸ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©\n",
        "!pip install dask[complete] pyarrow fastparquet tqdm --quiet\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "from time import perf_counter\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from google.colab import files # Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ù‚Ø³Ù… Kaggle API\n",
        "\n",
        "# ============================================================\n",
        "# 1ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Kaggle API Token (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙˆÙƒÙ† Ø§Ù„Ù…Ø±ÙÙˆØ¹)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 1ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Kaggle API Token ===\")\n",
        "\n",
        "# ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙˆÙƒÙ†\n",
        "kaggle_dir = os.path.join(os.path.expanduser('~'), '.kaggle')\n",
        "os.makedirs(kaggle_dir, exist_ok=True)\n",
        "token_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
        "uploaded_token_filename = 'kaggle.json'\n",
        "\n",
        "if os.path.exists(uploaded_token_filename):\n",
        "    print(f\"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù {uploaded_token_filename} Ø§Ù„Ù…Ø±ÙÙˆØ¹.\")\n",
        "    # Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­\n",
        "    !mv {uploaded_token_filename} {token_path}\n",
        "    # ØªØºÙŠÙŠØ± Ø£Ø°ÙˆÙ†Ø§Øª Ø§Ù„Ù…Ù„Ù Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø£Ù…Ø§Ù†\n",
        "    !chmod 600 {token_path}\n",
        "    print(\"âœ… ØªÙ… Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„Ù ÙˆØ¶Ø¨Ø· Ø§Ù„Ø£Ø°ÙˆÙ†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.\")\n",
        "\n",
        "    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„\n",
        "    print(\"\\nğŸ”¹ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Kaggle (Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£ÙˆÙ„ 5 Ù…Ø³Ø§Ø¨Ù‚Ø§Øª)...\")\n",
        "    !kaggle competitions list --max-entries 5\n",
        "else:\n",
        "    print(f\"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù {uploaded_token_filename} Ø§Ù„Ù…Ø±ÙÙˆØ¹ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2ï¸âƒ£ Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV ÙƒØ¨ÙŠØ± (~5GB)\n",
        "# (Ù†Ø³ØªØ®Ø¯Ù… Ù…Ù„Ù ÙˆÙ‡Ù…ÙŠ Ù„Ø£Ù† Ù…Ù„Ù 'gender_voice_dataset' ØµØºÙŠØ± Ø¬Ø¯Ù‹Ø§)\n",
        "# ============================================================\n",
        "\n",
        "base_dir = \"/content\"\n",
        "csv_path = os.path.join(base_dir, \"large_5gb_sample.csv\")\n",
        "\n",
        "# âœ³ï¸ Ø¹Ø¯Ù‘Ù„ Ø¥Ù„Ù‰ 28_000_000 Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ~5GB ÙØ¹Ù„ÙŠ\n",
        "# âŒ ØªÙ… ØªØµØ­ÙŠØ­ Ø§Ù„Ø®Ø·Ø£ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø±\n",
        "rows = 5_000_000 # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© (~0.9GB)\n",
        "# Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù 5GBØŒ Ù‚Ù… Ø¨ØªØºÙŠÙŠØ± Ø§Ù„Ù‚ÙŠÙ…Ø© Ø¥Ù„Ù‰: rows = 28_000_000\n",
        "\n",
        "cols = 5\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    print(\"\\nğŸ”¹ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù CSV ÙƒØ¨ÙŠØ±... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚ â³\")\n",
        "    np.random.seed(42)\n",
        "    df = pd.DataFrame({\n",
        "        'id': np.arange(rows),\n",
        "        'fare_amount': np.random.rand(rows) * 100, # Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ø­Ø³Ø§Ø¨Ù‡\n",
        "        'distance_km': np.random.rand(rows) * 50,\n",
        "        'passengers': np.random.randint(1, 5, size=rows),\n",
        "        'payment_type': np.random.choice(['cash', 'card'], size=rows)\n",
        "    })\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    size_gb = os.path.getsize(csv_path) / (1024**3)\n",
        "    print(f\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù: {csv_path} ({size_gb:.2f} GB)\")\n",
        "else:\n",
        "    size_gb = os.path.getsize(csv_path) / (1024**3)\n",
        "    print(f\"\\nâœ… Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§: {csv_path} ({size_gb:.2f} GB)\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: pandas.read_csv(chunksize)\n",
        "# (Ù…Ø¹Ø§Ù„Ø¬Ø© ØªØ¯Ø±ÙŠØ¬ÙŠØ© Ù„ØªØ¬Ù†Ø¨ Ù†ÙØ§Ø¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø©)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 3ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: pandas.read_csv(chunksize) ===\")\n",
        "\n",
        "start_pandas = perf_counter()\n",
        "total_sum = 0\n",
        "count = 0\n",
        "chunksize = 500_000 # Ø­Ø¬Ù… ÙƒÙ„ Ø¬Ø²Ø¡ ÙŠÙÙ‚Ø±Ø£\n",
        "\n",
        "total_chunks = int(np.ceil(rows / chunksize))\n",
        "for chunk in tqdm(pd.read_csv(csv_path, chunksize=chunksize, usecols=['fare_amount']),\n",
        "                 total=total_chunks, desc=\"Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ù€ Pandas\"):\n",
        "    s = chunk['fare_amount'].dropna()\n",
        "    total_sum += s.sum()\n",
        "    count += s.count()\n",
        "\n",
        "pandas_mean = total_sum / count\n",
        "end_pandas = perf_counter()\n",
        "pandas_time = end_pandas - start_pandas\n",
        "print(f\"ğŸ“Š Ø§Ù„Ù…ØªÙˆØ³Ø· (Pandas Chunk): {pandas_mean:.4f}\")\n",
        "print(f\"â±ï¸ Ø§Ù„Ø²Ù…Ù†: {pandas_time:.2f} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Dask DataFrame\n",
        "# (Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†ÙˆØ§Ø©)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 4ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Dask DataFrame ===\")\n",
        "\n",
        "start_dask = perf_counter()\n",
        "# ÙŠÙ‚Ø±Ø£ Dask Ø§Ù„Ù…Ù„Ù ÙˆÙŠÙ‚Ø³Ù…Ù‡ Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©\n",
        "ddf = dd.read_csv(csv_path, assume_missing=True, usecols=['fare_amount'])\n",
        "# .compute() ÙŠØ·Ù„Ù‚ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©\n",
        "dask_mean = ddf['fare_amount'].mean().compute()\n",
        "end_dask = perf_counter()\n",
        "dask_time = end_dask - start_dask\n",
        "\n",
        "print(f\"ğŸ“Š Ø§Ù„Ù…ØªÙˆØ³Ø· (Dask): {dask_mean:.4f}\")\n",
        "print(f\"â±ï¸ Ø§Ù„Ø²Ù…Ù†: {dask_time:.2f} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet (Ø¶ØºØ· ÙˆÙ‚Ø±Ø§Ø¡Ø© Ø£Ø³Ø±Ø¹)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 5ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet ===\")\n",
        "\n",
        "out_dir = os.path.join(base_dir, \"parquet_dataset\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# 5.1 Ø²Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„ (CSV => Parquet)\n",
        "start_convert = perf_counter()\n",
        "i = 0\n",
        "# Ù†Ø³ØªØ®Ø¯Ù… pandas.read_csv(chunksize) Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ù„Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù‚Ø¯ Ù„Ø§ ÙŠØªØ³Ø¹ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
        "for chunk in tqdm(pd.read_csv(csv_path, chunksize=1_000_000), desc=\"ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet\"):\n",
        "    fn = os.path.join(out_dir, f\"part_{i}.parquet\")\n",
        "    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¶ØºØ· Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ (Ø¹Ø§Ø¯Ø©Ù‹ Snappy) Ù…Ø¹ Parquet\n",
        "    chunk.to_parquet(fn, engine='pyarrow', index=False)\n",
        "    i += 1\n",
        "end_convert = perf_counter()\n",
        "parquet_convert_time = end_convert - start_convert\n",
        "\n",
        "# Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Parquet\n",
        "parquet_files = glob.glob(os.path.join(out_dir, \"*.parquet\"))\n",
        "total_parquet_size_mb = sum(os.path.getsize(f) for f in parquet_files) / (1024**2)\n",
        "\n",
        "print(f\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(parquet_files)} Ù…Ù„Ù Parquet\")\n",
        "print(f\"ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø¬Ù… (Parquet): {total_parquet_size_mb:.2f} MB\")\n",
        "print(f\"â±ï¸ Ø²Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„ (CSV -> Parquet): {parquet_convert_time:.2f} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "# 5.2 Ø²Ù…Ù† Ù‚Ø±Ø§Ø¡Ø© ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ù…Ù† Parquet Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask\n",
        "print(\"\\nğŸ”¹ Ù‚Ø±Ø§Ø¡Ø© ÙˆØ­Ø³Ø§Ø¨ Ù…Ù† Parquet (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask)...\")\n",
        "start_read_parquet = perf_counter()\n",
        "# Dask ÙŠÙ‚Ø±Ø£ Parquet Ø¨ÙØ¹Ø§Ù„ÙŠØ© ÙƒØ¨ÙŠØ±Ø© Ø¨ÙØ¶Ù„ Ø®Ø§ØµÙŠØ© Columnar Storage\n",
        "ddf_parquet = dd.read_parquet(out_dir, columns=['fare_amount'])\n",
        "parquet_mean = ddf_parquet['fare_amount'].mean().compute()\n",
        "end_read_parquet = perf_counter()\n",
        "parquet_read_time = end_read_parquet - start_read_parquet\n",
        "\n",
        "print(f\"ğŸ“Š Ø§Ù„Ù…ØªÙˆØ³Ø· (Dask Parquet): {parquet_mean:.4f}\")\n",
        "print(f\"â±ï¸ Ø²Ù…Ù† Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„Ø­Ø³Ø§Ø¨: {parquet_read_time:.2f} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "# ============================================================\n",
        "# 6ï¸âƒ£ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 6ï¸âƒ£ ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ===\")\n",
        "csv_size_mb = size_gb * 1024\n",
        "\n",
        "print(f\"Ø­Ø¬Ù… CSV Ø§Ù„Ø£ØµÙ„ÙŠ: {csv_size_mb:.2f} MB\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1 (Pandas Chunk): | Ø§Ù„Ù…ØªÙˆØ³Ø· {pandas_mean:.4f} | Ø§Ù„Ø²Ù…Ù† {pandas_time:.2f}s | Ø§Ù„ØªØ®Ø²ÙŠÙ†: {csv_size_mb:.2f} MB\")\n",
        "print(f\"Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2 (Dask CSV): Â  Â  | Ø§Ù„Ù…ØªÙˆØ³Ø· {dask_mean:.4f} | Ø§Ù„Ø²Ù…Ù† {dask_time:.2f}s | Ø§Ù„ØªØ®Ø²ÙŠÙ†: {csv_size_mb:.2f} MB\")\n",
        "print(f\"Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3 (Dask Parquet): | Ø§Ù„Ù…ØªÙˆØ³Ø· {parquet_mean:.4f} | Ø§Ù„Ø²Ù…Ù† (Ù‚Ø±Ø§Ø¡Ø©) {parquet_read_time:.2f}s | Ø§Ù„ØªØ®Ø²ÙŠÙ†: {total_parquet_size_mb:.2f} MB (Ù…Ø¶ØºÙˆØ·)\")\n",
        "print(f\"Ø²Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ: Â  Â  Â  Â  Â | - Â  Â  Â  Â  Â  Â  Â  Â  Â | Ø§Ù„Ø²Ù…Ù† (ØªØ­ÙˆÙŠÙ„) {parquet_convert_time:.2f}s\")\n",
        "\n",
        "print(\"\\nâœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù†Ø¬Ø§Ø­.\")"
      ]
    }
  ]
}
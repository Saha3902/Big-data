{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsR72X23bok33YFfpAgwkg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saha3902/Big-data/blob/main/big_data_tp02_%D9%85%D8%B9%D8%AF%D9%84\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0DYn6flBocY",
        "outputId": "cb7cb661-86d4-4763-b37e-44e228fc96f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "=== 1️⃣ إعداد Kaggle API Token ===\n",
            "⚠️ تحذير: لم يتم العثور على ملف kaggle.json المرفوع في المسار الحالي.\n",
            "\n",
            "✅ الملف موجود مسبقًا: /content/large_5gb_sample.csv (0.10 GB)\n",
            "\n",
            "=== 3️⃣ الطريقة 1: pandas.read_csv(chunksize) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "قراءة الملف بـ Pandas:  50%|█████     | 5/10 [00:00<00:00,  5.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 المتوسط (Pandas Chunk): 49.9919\n",
            "⏱️ الزمن: 0.91 ثانية\n",
            "\n",
            "=== 4️⃣ الطريقة 2: Dask DataFrame ===\n",
            "📊 المتوسط (Dask): 49.9919\n",
            "⏱️ الزمن: 2.03 ثانية\n",
            "\n",
            "=== 5️⃣ الطريقة 3: التحويل إلى Parquet ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "تحويل إلى Parquet: 3it [00:01,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ تم إنشاء 3 ملف Parquet\n",
            "📦 إجمالي الحجم (Parquet): 40.82 MB\n",
            "⏱️ زمن التحويل (CSV -> Parquet): 1.78 ثانية\n",
            "\n",
            "🔹 قراءة وحساب من Parquet (باستخدام Dask)...\n",
            "📊 المتوسط (Dask Parquet): 49.9919\n",
            "⏱️ زمن القراءة والحساب: 0.12 ثانية\n",
            "\n",
            "=== 6️⃣ 📊 مقارنة النتائج ===\n",
            "حجم CSV الأصلي: 98.04 MB\n",
            "----------------------------------------\n",
            "الطريقة 1 (Pandas Chunk): | المتوسط 49.9919 | الزمن 0.91s | التخزين: 98.04 MB\n",
            "الطريقة 2 (Dask CSV):     | المتوسط 49.9919 | الزمن 2.03s | التخزين: 98.04 MB\n",
            "الطريقة 3 (Dask Parquet): | المتوسط 49.9919 | الزمن (قراءة) 0.12s | التخزين: 40.82 MB (مضغوط)\n",
            "زمن تحويل إضافي:          | -                  | الزمن (تحويل) 1.78s\n",
            "\n",
            "✅ انتهى التنفيذ بنجاح.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# TP02: التعامل مع ملفات CSV ضخمة (حوالي 5GB) في Google Colab\n",
        "# ============================================================\n",
        "\n",
        "# ⚙️ تثبيت المكتبات اللازمة\n",
        "!pip install dask[complete] pyarrow fastparquet tqdm --quiet\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "from time import perf_counter\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from google.colab import files # للاستخدام في قسم Kaggle API\n",
        "\n",
        "# ============================================================\n",
        "# 1️⃣ إعداد Kaggle API Token (باستخدام التوكن المرفوع)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 1️⃣ إعداد Kaggle API Token ===\")\n",
        "\n",
        "# تحديد مسار ملف التوكن\n",
        "kaggle_dir = os.path.join(os.path.expanduser('~'), '.kaggle')\n",
        "os.makedirs(kaggle_dir, exist_ok=True)\n",
        "token_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
        "uploaded_token_filename = 'kaggle.json'\n",
        "\n",
        "if os.path.exists(uploaded_token_filename):\n",
        "    print(f\"✅ تم العثور على ملف {uploaded_token_filename} المرفوع.\")\n",
        "    # نقل الملف إلى المسار الصحيح\n",
        "    !mv {uploaded_token_filename} {token_path}\n",
        "    # تغيير أذونات الملف لضمان الأمان\n",
        "    !chmod 600 {token_path}\n",
        "    print(\"✅ تم نقل الملف وضبط الأذونات بنجاح.\")\n",
        "\n",
        "    # اختبار الاتصال\n",
        "    print(\"\\n🔹 اختبار الاتصال بـ Kaggle (قائمة بأول 5 مسابقات)...\")\n",
        "    !kaggle competitions list --max-entries 5\n",
        "else:\n",
        "    print(f\"⚠️ تحذير: لم يتم العثور على ملف {uploaded_token_filename} المرفوع في المسار الحالي.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2️⃣ إنشاء أو تحميل ملف CSV كبير (~5GB)\n",
        "# (نستخدم ملف وهمي لأن ملف 'gender_voice_dataset' صغير جدًا)\n",
        "# ============================================================\n",
        "\n",
        "base_dir = \"/content\"\n",
        "csv_path = os.path.join(base_dir, \"large_5gb_sample.csv\")\n",
        "\n",
        "# ✳️ عدّل إلى 28_000_000 للحصول على ~5GB فعلي\n",
        "# ❌ تم تصحيح الخطأ في هذا السطر\n",
        "rows = 5_000_000 # عدد الصفوف المستخدم في التجربة السريعة (~0.9GB)\n",
        "# لإنشاء ملف 5GB، قم بتغيير القيمة إلى: rows = 28_000_000\n",
        "\n",
        "cols = 5\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    print(\"\\n🔹 جاري إنشاء ملف CSV كبير... قد يستغرق هذا بضع دقائق ⏳\")\n",
        "    np.random.seed(42)\n",
        "    df = pd.DataFrame({\n",
        "        'id': np.arange(rows),\n",
        "        'fare_amount': np.random.rand(rows) * 100, # العمود الذي سيتم حسابه\n",
        "        'distance_km': np.random.rand(rows) * 50,\n",
        "        'passengers': np.random.randint(1, 5, size=rows),\n",
        "        'payment_type': np.random.choice(['cash', 'card'], size=rows)\n",
        "    })\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    size_gb = os.path.getsize(csv_path) / (1024**3)\n",
        "    print(f\"✅ تم إنشاء الملف: {csv_path} ({size_gb:.2f} GB)\")\n",
        "else:\n",
        "    size_gb = os.path.getsize(csv_path) / (1024**3)\n",
        "    print(f\"\\n✅ الملف موجود مسبقًا: {csv_path} ({size_gb:.2f} GB)\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3️⃣ الطريقة الأولى: pandas.read_csv(chunksize)\n",
        "# (معالجة تدريجية لتجنب نفاد الذاكرة)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 3️⃣ الطريقة 1: pandas.read_csv(chunksize) ===\")\n",
        "\n",
        "start_pandas = perf_counter()\n",
        "total_sum = 0\n",
        "count = 0\n",
        "chunksize = 500_000 # حجم كل جزء يُقرأ\n",
        "\n",
        "total_chunks = int(np.ceil(rows / chunksize))\n",
        "for chunk in tqdm(pd.read_csv(csv_path, chunksize=chunksize, usecols=['fare_amount']),\n",
        "                 total=total_chunks, desc=\"قراءة الملف بـ Pandas\"):\n",
        "    s = chunk['fare_amount'].dropna()\n",
        "    total_sum += s.sum()\n",
        "    count += s.count()\n",
        "\n",
        "pandas_mean = total_sum / count\n",
        "end_pandas = perf_counter()\n",
        "pandas_time = end_pandas - start_pandas\n",
        "print(f\"📊 المتوسط (Pandas Chunk): {pandas_mean:.4f}\")\n",
        "print(f\"⏱️ الزمن: {pandas_time:.2f} ثانية\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4️⃣ الطريقة الثانية: Dask DataFrame\n",
        "# (معالجة متوازية على مستوى النواة)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 4️⃣ الطريقة 2: Dask DataFrame ===\")\n",
        "\n",
        "start_dask = perf_counter()\n",
        "# يقرأ Dask الملف ويقسمه داخليًا إلى أجزاء للمعالجة المتوازية\n",
        "ddf = dd.read_csv(csv_path, assume_missing=True, usecols=['fare_amount'])\n",
        "# .compute() يطلق عملية الحساب المتوازية\n",
        "dask_mean = ddf['fare_amount'].mean().compute()\n",
        "end_dask = perf_counter()\n",
        "dask_time = end_dask - start_dask\n",
        "\n",
        "print(f\"📊 المتوسط (Dask): {dask_mean:.4f}\")\n",
        "print(f\"⏱️ الزمن: {dask_time:.2f} ثانية\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5️⃣ الطريقة الثالثة: التحويل إلى Parquet (ضغط وقراءة أسرع)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 5️⃣ الطريقة 3: التحويل إلى Parquet ===\")\n",
        "\n",
        "out_dir = os.path.join(base_dir, \"parquet_dataset\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# 5.1 زمن التحويل (CSV => Parquet)\n",
        "start_convert = perf_counter()\n",
        "i = 0\n",
        "# نستخدم pandas.read_csv(chunksize) للتحويل لأن الملف قد لا يتسع في الذاكرة\n",
        "for chunk in tqdm(pd.read_csv(csv_path, chunksize=1_000_000), desc=\"تحويل إلى Parquet\"):\n",
        "    fn = os.path.join(out_dir, f\"part_{i}.parquet\")\n",
        "    # استخدام الضغط الافتراضي (عادةً Snappy) مع Parquet\n",
        "    chunk.to_parquet(fn, engine='pyarrow', index=False)\n",
        "    i += 1\n",
        "end_convert = perf_counter()\n",
        "parquet_convert_time = end_convert - start_convert\n",
        "\n",
        "# حساب حجم Parquet\n",
        "parquet_files = glob.glob(os.path.join(out_dir, \"*.parquet\"))\n",
        "total_parquet_size_mb = sum(os.path.getsize(f) for f in parquet_files) / (1024**2)\n",
        "\n",
        "print(f\"✅ تم إنشاء {len(parquet_files)} ملف Parquet\")\n",
        "print(f\"📦 إجمالي الحجم (Parquet): {total_parquet_size_mb:.2f} MB\")\n",
        "print(f\"⏱️ زمن التحويل (CSV -> Parquet): {parquet_convert_time:.2f} ثانية\")\n",
        "\n",
        "# 5.2 زمن قراءة وحساب المتوسط من Parquet باستخدام Dask\n",
        "print(\"\\n🔹 قراءة وحساب من Parquet (باستخدام Dask)...\")\n",
        "start_read_parquet = perf_counter()\n",
        "# Dask يقرأ Parquet بفعالية كبيرة بفضل خاصية Columnar Storage\n",
        "ddf_parquet = dd.read_parquet(out_dir, columns=['fare_amount'])\n",
        "parquet_mean = ddf_parquet['fare_amount'].mean().compute()\n",
        "end_read_parquet = perf_counter()\n",
        "parquet_read_time = end_read_parquet - start_read_parquet\n",
        "\n",
        "print(f\"📊 المتوسط (Dask Parquet): {parquet_mean:.4f}\")\n",
        "print(f\"⏱️ زمن القراءة والحساب: {parquet_read_time:.2f} ثانية\")\n",
        "\n",
        "# ============================================================\n",
        "# 6️⃣ المقارنة النهائية\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 6️⃣ 📊 مقارنة النتائج ===\")\n",
        "csv_size_mb = size_gb * 1024\n",
        "\n",
        "print(f\"حجم CSV الأصلي: {csv_size_mb:.2f} MB\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"الطريقة 1 (Pandas Chunk): | المتوسط {pandas_mean:.4f} | الزمن {pandas_time:.2f}s | التخزين: {csv_size_mb:.2f} MB\")\n",
        "print(f\"الطريقة 2 (Dask CSV):     | المتوسط {dask_mean:.4f} | الزمن {dask_time:.2f}s | التخزين: {csv_size_mb:.2f} MB\")\n",
        "print(f\"الطريقة 3 (Dask Parquet): | المتوسط {parquet_mean:.4f} | الزمن (قراءة) {parquet_read_time:.2f}s | التخزين: {total_parquet_size_mb:.2f} MB (مضغوط)\")\n",
        "print(f\"زمن تحويل إضافي:          | -                  | الزمن (تحويل) {parquet_convert_time:.2f}s\")\n",
        "\n",
        "print(\"\\n✅ انتهى التنفيذ بنجاح.\")"
      ]
    }
  ]
}
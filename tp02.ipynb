{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Ø§Ù„Ø®Ù„ÙŠØ© 1: Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "# ===================================================================\n",
        "\n",
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "!pip install -q kaggle dask[complete]\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, time, gzip, shutil, glob, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "from google.colab import files\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“¦ TP02: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© (5GB+)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ====== Ø¥Ø¹Ø¯Ø§Ø¯ Kaggle API ======\n",
        "print(\"\\nğŸ” Ø¥Ø¹Ø¯Ø§Ø¯ Kaggle API...\")\n",
        "print(\"ğŸ“ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø±ÙØ¹ Ù…Ù„Ù kaggle.json:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "uploaded_filename = list(uploaded.keys())[0]\n",
        "if uploaded_filename != 'kaggle.json':\n",
        "    os.rename(uploaded_filename, 'kaggle.json')\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Kaggle API\")\n",
        "\n",
        "# ====== ØªØ­Ù…ÙŠÙ„ Dataset ======\n",
        "DATASET = \"mkechinov/ecommerce-behavior-data-from-multi-category-store\"\n",
        "print(f\"\\nâ³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„: {DATASET}\")\n",
        "!kaggle datasets download -d {DATASET}\n",
        "\n",
        "# ÙÙƒ Ø§Ù„Ø¶ØºØ·\n",
        "print(\"\\nâ³ Ø¬Ø§Ø±ÙŠ ÙÙƒ Ø§Ù„Ø¶ØºØ·...\")\n",
        "zip_file = glob.glob(\"*.zip\")[0]\n",
        "!unzip -q {zip_file}\n",
        "\n",
        "# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„Ù\n",
        "DATA_CSV = \"2019-Oct.csv\"\n",
        "file_size_gb = os.path.getsize(DATA_CSV) / (1024**3)\n",
        "print(f\"\\nâœ… Ø¬Ø§Ù‡Ø²! Ø§Ù„Ù…Ù„Ù: {DATA_CSV} ({file_size_gb:.2f} GB)\")\n",
        "!head -3 {DATA_CSV}\n",
        "\n",
        "# ====== ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø© ======\n",
        "def get_file_size_mb(path):\n",
        "    return os.path.getsize(path) / (1024**2) if os.path.exists(path) else None\n",
        "\n",
        "def measure_time(func, *args, **kwargs):\n",
        "    start = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    return result, time.time() - start\n",
        "\n",
        "print(\"\\nâœ… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙƒØªÙ…Ù„!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "iJiwArZrMIPc",
        "outputId": "0a1a48f1-121e-4590-9e8a-6685cbcd49c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“¦ TP02: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© (5GB+)\n",
            "============================================================\n",
            "\n",
            "ğŸ” Ø¥Ø¹Ø¯Ø§Ø¯ Kaggle API...\n",
            "ğŸ“ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø±ÙØ¹ Ù…Ù„Ù kaggle.json:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ede92a3-1498-41de-b056-3c653178dd37\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ede92a3-1498-41de-b056-3c653178dd37\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle(1).json to kaggle(1).json\n",
            "âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Kaggle API\n",
            "\n",
            "â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„: mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
            "Dataset URL: https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
            "License(s): copyright-authors\n",
            "Downloading ecommerce-behavior-data-from-multi-category-store.zip to /content\n",
            "100% 4.27G/4.29G [00:51<00:00, 252MB/s]\n",
            "100% 4.29G/4.29G [00:52<00:00, 88.5MB/s]\n",
            "\n",
            "â³ Ø¬Ø§Ø±ÙŠ ÙÙƒ Ø§Ù„Ø¶ØºØ·...\n",
            "\n",
            "âœ… Ø¬Ø§Ù‡Ø²! Ø§Ù„Ù…Ù„Ù: 2019-Oct.csv (5.28 GB)\n",
            "event_time,event_type,product_id,category_id,category_code,brand,price,user_id,user_session\n",
            "2019-10-01 00:00:00 UTC,view,44600062,2103807459595387724,,shiseido,35.79,541312140,72d76fde-8bb3-4e00-8c23-a032dfed738c\n",
            "2019-10-01 00:00:00 UTC,view,3900821,2053013552326770905,appliances.environment.water_heater,aqua,33.20,554748717,9333dfbd-b87a-4708-9857-6336556b0fcc\n",
            "\n",
            "âœ… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙƒØªÙ…Ù„!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Ø§Ù„Ø®Ù„ÙŠØ© 2: ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø«Ù„Ø§Ø« ÙˆØ§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
        "# ===================================================================\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ”¹ Ø¨Ø¯Ø¡ ØªÙ†ÙÙŠØ° Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø«Ù„Ø§Ø«\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ========== Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Pandas Chunks ==========\n",
        "print(\"\\nâ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Pandas Chunks...\")\n",
        "\n",
        "def method_pandas_chunks(file_path, chunksize=100000):\n",
        "    total = 0\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):\n",
        "        total += len(chunk)\n",
        "    return total\n",
        "\n",
        "rows_pandas, time_pandas = measure_time(method_pandas_chunks, DATA_CSV)\n",
        "print(f\"âœ… Pandas: {rows_pandas:,} ØµÙ ÙÙŠ {time_pandas:.2f}s ({rows_pandas/time_pandas:,.0f} ØµÙ/Ø«Ø§Ù†ÙŠØ©)\")\n",
        "\n",
        "results.append({\n",
        "    'method': 'Pandas Chunks',\n",
        "    'rows': rows_pandas,\n",
        "    'time_seconds': round(time_pandas, 2),\n",
        "    'speed': round(rows_pandas/time_pandas, 0)\n",
        "})\n",
        "\n",
        "# ========== Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Dask ==========\n",
        "print(\"\\nâ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Dask...\")\n",
        "\n",
        "def method_dask(file_path):\n",
        "    ddf = dd.read_csv(file_path, blocksize=\"64MB\", assume_missing=True)\n",
        "    return ddf.shape[0].compute()\n",
        "\n",
        "rows_dask, time_dask = measure_time(method_dask, DATA_CSV)\n",
        "print(f\"âœ… Dask: {rows_dask:,} ØµÙ ÙÙŠ {time_dask:.2f}s ({rows_dask/time_dask:,.0f} ØµÙ/Ø«Ø§Ù†ÙŠØ©)\")\n",
        "\n",
        "results.append({\n",
        "    'method': 'Dask',\n",
        "    'rows': rows_dask,\n",
        "    'time_seconds': round(time_dask, 2),\n",
        "    'speed': round(rows_dask/time_dask, 0)\n",
        "})\n",
        "\n",
        "# ========== Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Compression ==========\n",
        "print(\"\\nâ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Compression...\")\n",
        "\n",
        "COMPRESSED_FILE = DATA_CSV + \".gz\"\n",
        "\n",
        "# Ø¶ØºØ· Ø§Ù„Ù…Ù„Ù\n",
        "def compress_file(inp, out):\n",
        "    with open(inp, 'rb') as f_in, gzip.open(out, 'wb', compresslevel=6) as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "_, time_compress = measure_time(compress_file, DATA_CSV, COMPRESSED_FILE)\n",
        "\n",
        "original_mb = get_file_size_mb(DATA_CSV)\n",
        "compressed_mb = get_file_size_mb(COMPRESSED_FILE)\n",
        "compression_ratio = ((original_mb - compressed_mb) / original_mb) * 100\n",
        "\n",
        "print(f\"   Ø¶ØºØ·: {original_mb:.2f} MB â†’ {compressed_mb:.2f} MB ({compression_ratio:.1f}% ØªÙˆÙÙŠØ±) ÙÙŠ {time_compress:.2f}s\")\n",
        "\n",
        "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¶ØºÙˆØ·\n",
        "def method_compressed(file_path, chunksize=100000):\n",
        "    total = 0\n",
        "    for chunk in pd.read_csv(file_path, compression='gzip', chunksize=chunksize, low_memory=False):\n",
        "        total += len(chunk)\n",
        "    return total\n",
        "\n",
        "rows_comp, time_read = measure_time(method_compressed, COMPRESSED_FILE)\n",
        "total_time = time_compress + time_read\n",
        "\n",
        "print(f\"âœ… Compression: {rows_comp:,} ØµÙ ÙÙŠ {total_time:.2f}s (Ø¶ØºØ·: {time_compress:.2f}s + Ù‚Ø±Ø§Ø¡Ø©: {time_read:.2f}s)\")\n",
        "\n",
        "results.append({\n",
        "    'method': 'Compression',\n",
        "    'rows': rows_comp,\n",
        "    'time_seconds': round(total_time, 2),\n",
        "    'speed': round(rows_comp/total_time, 0),\n",
        "    'original_mb': round(original_mb, 2),\n",
        "    'compressed_mb': round(compressed_mb, 2),\n",
        "    'compression_ratio': round(compression_ratio, 2)\n",
        "})\n",
        "\n",
        "# ========== Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ==========\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('comparison_results.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\")\n",
        "print(\"=\"*60)\n",
        "print(results_df[['method', 'rows', 'time_seconds', 'speed']].to_string(index=False))\n",
        "\n",
        "print(\"\\nğŸ’¾ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø³Ø§Ø­Ø©:\")\n",
        "print(f\"   Ø§Ù„Ø£ØµÙ„ÙŠ: {original_mb:.2f} MB\")\n",
        "print(f\"   Ø§Ù„Ù…Ø¶ØºÙˆØ·: {compressed_mb:.2f} MB\")\n",
        "print(f\"   Ø§Ù„ØªÙˆÙÙŠØ±: {compression_ratio:.1f}%\")\n",
        "\n",
        "fastest = results_df.loc[results_df['time_seconds'].idxmin(), 'method']\n",
        "print(f\"\\nğŸ† Ø£Ø³Ø±Ø¹ Ø·Ø±ÙŠÙ‚Ø©: {fastest}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czkboP90MmWU",
        "outputId": "54a128c6-42df-46ef-ba9b-f2b99164304b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ”¹ Ø¨Ø¯Ø¡ ØªÙ†ÙÙŠØ° Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø«Ù„Ø§Ø«\n",
            "============================================================\n",
            "\n",
            "â³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Pandas Chunks...\n",
            "âœ… Pandas: 42,448,764 ØµÙ ÙÙŠ 119.42s (355,469 ØµÙ/Ø«Ø§Ù†ÙŠØ©)\n",
            "\n",
            "â³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Dask...\n",
            "âœ… Dask: 42,448,764 ØµÙ ÙÙŠ 113.44s (374,206 ØµÙ/Ø«Ø§Ù†ÙŠØ©)\n",
            "\n",
            "â³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Compression...\n",
            "   Ø¶ØºØ·: 5406.01 MB â†’ 1661.23 MB (69.3% ØªÙˆÙÙŠØ±) ÙÙŠ 308.61s\n",
            "âœ… Compression: 42,448,764 ØµÙ ÙÙŠ 462.03s (Ø¶ØºØ·: 308.61s + Ù‚Ø±Ø§Ø¡Ø©: 153.42s)\n",
            "\n",
            "============================================================\n",
            "ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
            "============================================================\n",
            "       method     rows  time_seconds    speed\n",
            "Pandas Chunks 42448764        119.42 355469.0\n",
            "         Dask 42448764        113.44 374206.0\n",
            "  Compression 42448764        462.03  91875.0\n",
            "\n",
            "ğŸ’¾ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø³Ø§Ø­Ø©:\n",
            "   Ø§Ù„Ø£ØµÙ„ÙŠ: 5406.01 MB\n",
            "   Ø§Ù„Ù…Ø¶ØºÙˆØ·: 1661.23 MB\n",
            "   Ø§Ù„ØªÙˆÙÙŠØ±: 69.3%\n",
            "\n",
            "ğŸ† Ø£Ø³Ø±Ø¹ Ø·Ø±ÙŠÙ‚Ø©: Dask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Ø§Ù„Ø®Ù„ÙŠØ© 3: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "# ===================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ====== Ø¥Ù†Ø´Ø§Ø¡ README.md ======\n",
        "readme_text = f\"\"\"# TP02: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© (5GB+)\n",
        "\n",
        "## ÙˆØµÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹\n",
        "ØªØ·Ø¨ÙŠÙ‚ Ø«Ù„Ø§Ø« Ø·Ø±Ù‚ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª CSV Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Python:\n",
        "1. Pandas Chunks: Ù‚Ø±Ø§Ø¡Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª\n",
        "2. Dask: Ø­ÙˆØ³Ø¨Ø© Ù…ÙˆØ²Ø¹Ø©\n",
        "3. Compression: Ø¶ØºØ· + Ù‚Ø±Ø§Ø¡Ø©\n",
        "\n",
        "## Dataset Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…\n",
        "- Ø§Ù„Ù…ØµØ¯Ø±: eCommerce Behavior Data\n",
        "- Ø§Ù„Ù…Ù„Ù: {DATA_CSV}\n",
        "- Ø§Ù„Ø­Ø¬Ù…: {file_size_gb:.2f} GB\n",
        "- Ø§Ù„ØµÙÙˆÙ: {rows_pandas:,}\n",
        "\n",
        "## Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "\n",
        "### Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡\n",
        "| Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© | Ø§Ù„ÙˆÙ‚Øª (Ø«Ø§Ù†ÙŠØ©) | Ø§Ù„Ø³Ø±Ø¹Ø© (ØµÙ/Ø«Ø§Ù†ÙŠØ©) |\n",
        "|---------|---------------|-------------------|\n",
        "| Pandas Chunks | {time_pandas:.2f} | {rows_pandas/time_pandas:,.0f} |\n",
        "| Dask | {time_dask:.2f} | {rows_dask/time_dask:,.0f} |\n",
        "| Compression | {total_time:.2f} | {rows_comp/total_time:,.0f} |\n",
        "\n",
        "### Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø³Ø§Ø­Ø©\n",
        "- Ø§Ù„Ø£ØµÙ„ÙŠ: {original_mb:,.2f} MB\n",
        "- Ø§Ù„Ù…Ø¶ØºÙˆØ·: {compressed_mb:,.2f} MB\n",
        "- Ø§Ù„ØªÙˆÙÙŠØ±: {compression_ratio:.1f}%\n",
        "\n",
        "## Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª\n",
        "- Ø£Ø³Ø±Ø¹ Ø·Ø±ÙŠÙ‚Ø©: {fastest}\n",
        "- Ø§Ù„Ø¶ØºØ· ÙŠÙˆÙØ± {compression_ratio:.1f}% Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø­Ø©\n",
        "- ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ù…Ø¹ÙŠÙ†\n",
        "\n",
        "## Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª\n",
        "pandas>=1.3.0\n",
        "dask[complete]>=2021.10.0\n",
        "kaggle>=1.5.0\n",
        "\n",
        "## Ø§Ù„ØªØ§Ø±ÙŠØ®\n",
        "{pd.Timestamp.now().strftime('%Y-%m-%d')}\n",
        "\"\"\"\n",
        "\n",
        "with open('README.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(readme_text)\n",
        "\n",
        "# ====== Ø¥Ù†Ø´Ø§Ø¡ requirements.txt ======\n",
        "requirements_text = \"\"\"pandas>=1.3.0\n",
        "dask[complete]>=2021.10.0\n",
        "kaggle>=1.5.0\n",
        "numpy>=1.20.0\n",
        "\"\"\"\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements_text)\n",
        "\n",
        "print(\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ README.md Ùˆ requirements.txt\")\n",
        "\n",
        "# ====== Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© ======\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ˆ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©\")\n",
        "print(\"=\"*60)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# ====== Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ======\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø³Ø¨ÙŠ:\")\n",
        "print(f\"   â€¢ Pandas: {time_pandas:.2f}s (Ø§Ù„Ø£Ø³Ø§Ø³)\")\n",
        "print(f\"   â€¢ Dask: {time_dask:.2f}s ({(time_dask/time_pandas-1)*100:+.1f}%)\")\n",
        "print(f\"   â€¢ Compression: {total_time:.2f}s ({(total_time/time_pandas-1)*100:+.1f}%)\")\n",
        "\n",
        "print(f\"\\nğŸ’¾ ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©:\")\n",
        "print(f\"   â€¢ Ø§Ù„ØªÙˆÙÙŠØ±: {original_mb - compressed_mb:,.2f} MB ({compression_ratio:.1f}%)\")\n",
        "\n",
        "print(\"\\nâœ… Ø§Ù„ØªÙˆØµÙŠØ§Øª:\")\n",
        "print(\"   1. Ù„Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰: Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ø±Ø¹ Ø·Ø±ÙŠÙ‚Ø©\")\n",
        "print(\"   2. Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©: Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¶ØºØ·\")\n",
        "print(\"   3. Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ø¬Ø¯Ø§Ù‹: Ø§Ø³ØªØ®Ø¯Ù… Dask\")\n",
        "print(\"   4. Ù„Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©: Pandas Chunks Ù…ØªÙˆØ§Ø²Ù†Ø©\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml543ucwQ8Il",
        "outputId": "2cd11ed6-9ef8-442f-addd-f26026dca3b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ\n",
            "============================================================\n",
            "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ README.md Ùˆ requirements.txt\n",
            "\n",
            "============================================================\n",
            "ğŸ“ˆ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©\n",
            "============================================================\n",
            "       method     rows  time_seconds    speed  original_mb  compressed_mb  compression_ratio\n",
            "Pandas Chunks 42448764        119.42 355469.0          NaN            NaN                NaN\n",
            "         Dask 42448764        113.44 374206.0          NaN            NaN                NaN\n",
            "  Compression 42448764        462.03  91875.0      5406.01        1661.23              69.27\n",
            "\n",
            "============================================================\n",
            "ğŸ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø³Ø¨ÙŠ:\n",
            "   â€¢ Pandas: 119.42s (Ø§Ù„Ø£Ø³Ø§Ø³)\n",
            "   â€¢ Dask: 113.44s (-5.0%)\n",
            "   â€¢ Compression: 462.03s (+286.9%)\n",
            "\n",
            "ğŸ’¾ ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©:\n",
            "   â€¢ Ø§Ù„ØªÙˆÙÙŠØ±: 3,744.78 MB (69.3%)\n",
            "\n",
            "âœ… Ø§Ù„ØªÙˆØµÙŠØ§Øª:\n",
            "   1. Ù„Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰: Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ø±Ø¹ Ø·Ø±ÙŠÙ‚Ø©\n",
            "   2. Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©: Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¶ØºØ·\n",
            "   3. Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ø¬Ø¯Ø§Ù‹: Ø§Ø³ØªØ®Ø¯Ù… Dask\n",
            "   4. Ù„Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©: Pandas Chunks Ù…ØªÙˆØ§Ø²Ù†Ø©\n"
          ]
        }
      ]
    }
  ]
}
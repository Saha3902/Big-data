{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg8YicCCghOX3yw7Bb30Vp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saha3902/Big-data/blob/main/tp_Big_data02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TP02: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª CSV Ø¶Ø®Ù…Ø© (Ø­ÙˆØ§Ù„ÙŠ 5GB) ÙÙŠ Google Colab\n",
        "# ============================================================\n",
        "\n",
        "!pip install dask[complete] pyarrow fastparquet tqdm --quiet\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "from time import perf_counter\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "# ============================================================\n",
        "# 1ï¸âƒ£ Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV ÙƒØ¨ÙŠØ± (~5GB)\n",
        "# ============================================================\n",
        "\n",
        "base_dir = \"/content\"\n",
        "csv_path = os.path.join(base_dir, \"large_5gb_sample.csv\")\n",
        "\n",
        "# âœ³ï¸ Ù„ØªÙ‚Ù„ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙÙŠ Colab ÙŠÙ…ÙƒÙ† ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ù…Ø¤Ù‚ØªÙ‹Ø§ (Ù…Ø«Ù„Ø§Ù‹ 5 Ù…Ù„ÙŠÙˆÙ† Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 28 Ù…Ù„ÙŠÙˆÙ†)\n",
        "rows = 5_000_000   # Ø¹Ø¯Ù‘Ù„ Ø¥Ù„Ù‰ 28_000_000 Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ~5GB ÙØ¹Ù„ÙŠ\n",
        "cols = 5\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    print(\"ğŸ”¹ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù CSV ÙƒØ¨ÙŠØ±... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚ â³\")\n",
        "    np.random.seed(42)\n",
        "    df = pd.DataFrame({\n",
        "        'id': np.arange(rows),\n",
        "        'fare_amount': np.random.rand(rows) * 100,\n",
        "        'distance_km': np.random.rand(rows) * 50,\n",
        "        'passengers': np.random.randint(1, 5, size=rows),\n",
        "        'payment_type': np.random.choice(['cash', 'card'], size=rows)\n",
        "    })\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    size_gb = os.path.getsize(csv_path) / (1024**3)\n",
        "    print(f\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù: {csv_path} ({size_gb:.2f} GB)\")\n",
        "else:\n",
        "    print(f\"âœ… Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§: {csv_path}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: pandas.read_csv(chunksize)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: pandas.read_csv(chunksize) ===\")\n",
        "\n",
        "start = perf_counter()\n",
        "total_sum = 0\n",
        "count = 0\n",
        "chunksize = 500_000  # Ø­Ø¬Ù… ÙƒÙ„ Ø¬Ø²Ø¡\n",
        "\n",
        "# âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… tqdm Ù„Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù… ÙÙŠ Colab\n",
        "total_chunks = int(np.ceil(rows / chunksize))\n",
        "for chunk in tqdm(pd.read_csv(csv_path, chunksize=chunksize, usecols=['fare_amount']),\n",
        "                  total=total_chunks, desc=\"Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ù€ Pandas\"):\n",
        "    s = chunk['fare_amount'].dropna()\n",
        "    total_sum += s.sum()\n",
        "    count += s.count()\n",
        "\n",
        "pandas_mean = total_sum / count\n",
        "end = perf_counter()\n",
        "pandas_time = end - start\n",
        "print(f\"ğŸ“Š Ø§Ù„Ù…ØªÙˆØ³Ø· (pandas): {pandas_mean:.4f}\")\n",
        "print(f\"â±ï¸ Ø§Ù„Ø²Ù…Ù†: {pandas_time:.2f} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Dask DataFrame\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Dask DataFrame ===\")\n",
        "\n",
        "start = perf_counter()\n",
        "ddf = dd.read_csv(csv_path, assume_missing=True, usecols=['fare_amount'])\n",
        "dask_mean = ddf['fare_amount'].mean().compute()\n",
        "end = perf_counter()\n",
        "dask_time = end - start\n",
        "\n",
        "print(f\" Ø§Ù„Ù…ØªÙˆØ³Ø· (Dask): {dask_mean:.4f}\")\n",
        "print(f\" Ø§Ù„Ø²Ù…Ù†: {dask_time:.2f} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4ï¸âƒ£ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet (Ø¶ØºØ· ÙˆÙ‚Ø±Ø§Ø¡Ø© Ø£Ø³Ø±Ø¹)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: ØªØ­ÙˆÙŠÙ„ CSV Ø¥Ù„Ù‰ Parquet ===\")\n",
        "\n",
        "out_dir = os.path.join(base_dir, \"parquet_dataset\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "start = perf_counter()\n",
        "i = 0\n",
        "for chunk in tqdm(pd.read_csv(csv_path, chunksize=1_000_000),\n",
        "                  desc=\"ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet\"):\n",
        "    fn = os.path.join(out_dir, f\"part_{i}.parquet\")\n",
        "    chunk.to_parquet(fn, engine='pyarrow', index=False)\n",
        "    i += 1\n",
        "end = perf_counter()\n",
        "parquet_time = end - start\n",
        "\n",
        "parquet_files = glob.glob(os.path.join(out_dir, \"*.parquet\"))\n",
        "total_parquet_size = sum(os.path.getsize(f) for f in parquet_files) / (1024**2)\n",
        "\n",
        "print(f\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(parquet_files)} Ù…Ù„Ù Parquet\")\n",
        "print(f\"ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø¬Ù…: {total_parquet_size:.2f} MB\")\n",
        "print(f\"â±ï¸ Ø²Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„: {parquet_time:.2f} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5ï¸âƒ£ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ===\")\n",
        "csv_size = os.path.getsize(csv_path) / (1024**3)\n",
        "print(f\"Ø­Ø¬Ù… CSV Ø§Ù„Ø£ØµÙ„ÙŠ: {csv_size:.2f} GB\")\n",
        "print(f\"Pandas: Ø§Ù„Ù…ØªÙˆØ³Ø· {pandas_mean:.4f} - Ø§Ù„Ø²Ù…Ù† {pandas_time:.2f}s\")\n",
        "print(f\"Dask:   Ø§Ù„Ù…ØªÙˆØ³Ø· {dask_mean:.4f} - Ø§Ù„Ø²Ù…Ù† {dask_time:.2f}s\")\n",
        "print(f\"Parquet Ø§Ù„Ø­Ø¬Ù…: {total_parquet_size/1024:.2f} GB - Ø²Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„: {parquet_time:.2f}s\")\n",
        "\n",
        "print(\"\\nâœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù†Ø¬Ø§Ø­.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT9tyW9r-AFJ",
        "outputId": "e9ea16f4-95ae-4072-d869-dcf439f5a112"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hğŸ”¹ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù CSV ÙƒØ¨ÙŠØ±... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚ â³\n",
            "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù: /content/large_5gb_sample.csv (0.24 GB)\n",
            "\n",
            "=== Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: pandas.read_csv(chunksize) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ù€ Pandas: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Ø§Ù„Ù…ØªÙˆØ³Ø· (pandas): 49.9987\n",
            "â±ï¸ Ø§Ù„Ø²Ù…Ù†: 2.43 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "=== Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Dask DataFrame ===\n",
            " Ø§Ù„Ù…ØªÙˆØ³Ø· (Dask): 49.9987\n",
            " Ø§Ù„Ø²Ù…Ù†: 4.30 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "=== Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: ØªØ­ÙˆÙŠÙ„ CSV Ø¥Ù„Ù‰ Parquet ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet: 5it [00:04,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 5 Ù…Ù„Ù Parquet\n",
            "ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø¬Ù…: 101.21 MB\n",
            "â±ï¸ Ø²Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„: 4.56 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "=== ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ===\n",
            "Ø­Ø¬Ù… CSV Ø§Ù„Ø£ØµÙ„ÙŠ: 0.24 GB\n",
            "Pandas: Ø§Ù„Ù…ØªÙˆØ³Ø· 49.9987 - Ø§Ù„Ø²Ù…Ù† 2.43s\n",
            "Dask:   Ø§Ù„Ù…ØªÙˆØ³Ø· 49.9987 - Ø§Ù„Ø²Ù…Ù† 4.30s\n",
            "Parquet Ø§Ù„Ø­Ø¬Ù…: 0.10 GB - Ø²Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„: 4.56s\n",
            "\n",
            "âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù†Ø¬Ø§Ø­.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe93aNruZe30c1nr6vNSS6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saha3902/Big-data/blob/main/tp_Big_data02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xWRDm1CaCHS",
        "outputId": "42c5c07f-1443-496f-d252-a7124d61f0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " File already exists: large_sample.csv\n",
            "\n",
            "=== Method 1: pandas.read_csv(chunksize) ===\n",
            " pandas mean(fare_amount): 49.9978\n",
            " Time: 0.88 seconds\n",
            "\n",
            "=== Method 2: Dask DataFrame ===\n",
            " Dask mean(fare_amount): 49.9978\n",
            " Time: 1.17 seconds\n",
            "\n",
            "=== Method 3: Convert CSV -> Parquet ===\n",
            " Parquet dataset created with 6 files\n",
            "Total Parquet size: 63.13 MB\n",
            " Conversion time: 1.71 seconds\n",
            "\n",
            "=== 📊 Summary Comparison ===\n",
            "Original CSV size: 146.58 MB\n",
            "Pandas chunks mean: 49.9978, time: 0.88s\n",
            "Dask mean: 49.9978, time: 1.17s\n",
            "Parquet total size: 63.13 MB, conversion time: 1.71s\n",
            "\n",
            " Done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "from time import perf_counter\n",
        "import glob\n",
        "\n",
        "\n",
        "# 1. توليد ملف CSV كبير نسبيًا (لتجربة حقيقية)\n",
        "\n",
        "csv_path = \"large_sample.csv\"\n",
        "rows = 3_000_000  # عدد الأسطر (3 مليون = ~100MB تقريباً)\n",
        "cols = 5\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    print(\"🔹 Generating sample CSV file...\")\n",
        "    np.random.seed(42)\n",
        "    df = pd.DataFrame({\n",
        "        'id': np.arange(rows),\n",
        "        'fare_amount': np.random.rand(rows) * 100,\n",
        "        'distance_km': np.random.rand(rows) * 50,\n",
        "        'passengers': np.random.randint(1, 5, size=rows),\n",
        "        'payment_type': np.random.choice(['cash', 'card'], size=rows)\n",
        "    })\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\" File created: {csv_path} ({os.path.getsize(csv_path)/(1024**2):.1f} MB)\")\n",
        "else:\n",
        "    print(f\" File already exists: {csv_path}\")\n",
        "\n",
        "\n",
        "# 2. الطريقة الأولى: pandas + chunksize\n",
        "\n",
        "print(\"\\n=== Method 1: pandas.read_csv(chunksize) ===\")\n",
        "\n",
        "start = perf_counter()\n",
        "total_sum = 0\n",
        "count = 0\n",
        "chunksize = 200_000\n",
        "\n",
        "for chunk in pd.read_csv(csv_path, chunksize=chunksize, usecols=['fare_amount']):\n",
        "    s = chunk['fare_amount'].dropna()\n",
        "    total_sum += s.sum()\n",
        "    count += s.count()\n",
        "\n",
        "pandas_mean = total_sum / count\n",
        "end = perf_counter()\n",
        "pandas_time = end - start\n",
        "print(f\" pandas mean(fare_amount): {pandas_mean:.4f}\")\n",
        "print(f\" Time: {pandas_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# 3. الطريقة الثانية: Dask DataFrame\n",
        "\n",
        "print(\"\\n=== Method 2: Dask DataFrame ===\")\n",
        "\n",
        "start = perf_counter()\n",
        "ddf = dd.read_csv(csv_path, assume_missing=True, usecols=['fare_amount'])\n",
        "dask_mean = ddf['fare_amount'].mean().compute()\n",
        "end = perf_counter()\n",
        "dask_time = end - start\n",
        "\n",
        "print(f\" Dask mean(fare_amount): {dask_mean:.4f}\")\n",
        "print(f\" Time: {dask_time:.2f} seconds\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. الطريقة الثالثة: التحويل إلى Parquet (ضغط وقراءة أسرع)\n",
        "# ------------------------------------------------\n",
        "print(\"\\n=== Method 3: Convert CSV -> Parquet ===\")\n",
        "\n",
        "out_dir = \"parquet_dataset\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "start = perf_counter()\n",
        "i = 0\n",
        "for chunk in pd.read_csv(csv_path, chunksize=500_000):\n",
        "    fn = os.path.join(out_dir, f\"part_{i}.parquet\")\n",
        "    chunk.to_parquet(fn, engine='pyarrow', index=False)\n",
        "    i += 1\n",
        "end = perf_counter()\n",
        "parquet_time = end - start\n",
        "\n",
        "parquet_files = glob.glob(os.path.join(out_dir, \"*.parquet\"))\n",
        "total_parquet_size = sum(os.path.getsize(f) for f in parquet_files) / (1024**2)\n",
        "\n",
        "print(f\" Parquet dataset created with {len(parquet_files)} files\")\n",
        "print(f\"Total Parquet size: {total_parquet_size:.2f} MB\")\n",
        "print(f\" Conversion time: {parquet_time:.2f} seconds\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. المقارنة النهائية\n",
        "# ------------------------------------------------\n",
        "print(\"\\n=== 📊 Summary Comparison ===\")\n",
        "csv_size = os.path.getsize(csv_path) / (1024**2)\n",
        "print(f\"Original CSV size: {csv_size:.2f} MB\")\n",
        "print(f\"Pandas chunks mean: {pandas_mean:.4f}, time: {pandas_time:.2f}s\")\n",
        "print(f\"Dask mean: {dask_mean:.4f}, time: {dask_time:.2f}s\")\n",
        "print(f\"Parquet total size: {total_parquet_size:.2f} MB, conversion time: {parquet_time:.2f}s\")\n",
        "print(\"\\n Done.\")\n"
      ]
    }
  ]
}
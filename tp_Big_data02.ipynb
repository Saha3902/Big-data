{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg8YicCCghOX3yw7Bb30Vp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saha3902/Big-data/blob/main/tp_Big_data02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TP02: التعامل مع ملفات CSV ضخمة (حوالي 5GB) في Google Colab\n",
        "# ============================================================\n",
        "\n",
        "!pip install dask[complete] pyarrow fastparquet tqdm --quiet\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "from time import perf_counter\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "# ============================================================\n",
        "# 1️⃣ إنشاء أو تحميل ملف CSV كبير (~5GB)\n",
        "# ============================================================\n",
        "\n",
        "base_dir = \"/content\"\n",
        "csv_path = os.path.join(base_dir, \"large_5gb_sample.csv\")\n",
        "\n",
        "# ✳️ لتقليل وقت التجربة في Colab يمكن تقليل عدد الصفوف مؤقتًا (مثلاً 5 مليون بدلاً من 28 مليون)\n",
        "rows = 5_000_000   # عدّل إلى 28_000_000 للحصول على ~5GB فعلي\n",
        "cols = 5\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    print(\"🔹 جاري إنشاء ملف CSV كبير... قد يستغرق هذا بضع دقائق ⏳\")\n",
        "    np.random.seed(42)\n",
        "    df = pd.DataFrame({\n",
        "        'id': np.arange(rows),\n",
        "        'fare_amount': np.random.rand(rows) * 100,\n",
        "        'distance_km': np.random.rand(rows) * 50,\n",
        "        'passengers': np.random.randint(1, 5, size=rows),\n",
        "        'payment_type': np.random.choice(['cash', 'card'], size=rows)\n",
        "    })\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    size_gb = os.path.getsize(csv_path) / (1024**3)\n",
        "    print(f\"✅ تم إنشاء الملف: {csv_path} ({size_gb:.2f} GB)\")\n",
        "else:\n",
        "    print(f\"✅ الملف موجود مسبقًا: {csv_path}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2️⃣ الطريقة الأولى: pandas.read_csv(chunksize)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== الطريقة 1: pandas.read_csv(chunksize) ===\")\n",
        "\n",
        "start = perf_counter()\n",
        "total_sum = 0\n",
        "count = 0\n",
        "chunksize = 500_000  # حجم كل جزء\n",
        "\n",
        "# ✅ استخدام tqdm لشريط التقدم في Colab\n",
        "total_chunks = int(np.ceil(rows / chunksize))\n",
        "for chunk in tqdm(pd.read_csv(csv_path, chunksize=chunksize, usecols=['fare_amount']),\n",
        "                  total=total_chunks, desc=\"قراءة الملف بـ Pandas\"):\n",
        "    s = chunk['fare_amount'].dropna()\n",
        "    total_sum += s.sum()\n",
        "    count += s.count()\n",
        "\n",
        "pandas_mean = total_sum / count\n",
        "end = perf_counter()\n",
        "pandas_time = end - start\n",
        "print(f\"📊 المتوسط (pandas): {pandas_mean:.4f}\")\n",
        "print(f\"⏱️ الزمن: {pandas_time:.2f} ثانية\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3️⃣ الطريقة الثانية: Dask DataFrame\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== الطريقة 2: Dask DataFrame ===\")\n",
        "\n",
        "start = perf_counter()\n",
        "ddf = dd.read_csv(csv_path, assume_missing=True, usecols=['fare_amount'])\n",
        "dask_mean = ddf['fare_amount'].mean().compute()\n",
        "end = perf_counter()\n",
        "dask_time = end - start\n",
        "\n",
        "print(f\" المتوسط (Dask): {dask_mean:.4f}\")\n",
        "print(f\" الزمن: {dask_time:.2f} ثانية\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4️⃣ الطريقة الثالثة: التحويل إلى Parquet (ضغط وقراءة أسرع)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== الطريقة 3: تحويل CSV إلى Parquet ===\")\n",
        "\n",
        "out_dir = os.path.join(base_dir, \"parquet_dataset\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "start = perf_counter()\n",
        "i = 0\n",
        "for chunk in tqdm(pd.read_csv(csv_path, chunksize=1_000_000),\n",
        "                  desc=\"تحويل إلى Parquet\"):\n",
        "    fn = os.path.join(out_dir, f\"part_{i}.parquet\")\n",
        "    chunk.to_parquet(fn, engine='pyarrow', index=False)\n",
        "    i += 1\n",
        "end = perf_counter()\n",
        "parquet_time = end - start\n",
        "\n",
        "parquet_files = glob.glob(os.path.join(out_dir, \"*.parquet\"))\n",
        "total_parquet_size = sum(os.path.getsize(f) for f in parquet_files) / (1024**2)\n",
        "\n",
        "print(f\"✅ تم إنشاء {len(parquet_files)} ملف Parquet\")\n",
        "print(f\"📦 إجمالي الحجم: {total_parquet_size:.2f} MB\")\n",
        "print(f\"⏱️ زمن التحويل: {parquet_time:.2f} ثانية\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5️⃣ المقارنة النهائية\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== 📊 مقارنة النتائج ===\")\n",
        "csv_size = os.path.getsize(csv_path) / (1024**3)\n",
        "print(f\"حجم CSV الأصلي: {csv_size:.2f} GB\")\n",
        "print(f\"Pandas: المتوسط {pandas_mean:.4f} - الزمن {pandas_time:.2f}s\")\n",
        "print(f\"Dask:   المتوسط {dask_mean:.4f} - الزمن {dask_time:.2f}s\")\n",
        "print(f\"Parquet الحجم: {total_parquet_size/1024:.2f} GB - زمن التحويل: {parquet_time:.2f}s\")\n",
        "\n",
        "print(\"\\n✅ انتهى التنفيذ بنجاح.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT9tyW9r-AFJ",
        "outputId": "e9ea16f4-95ae-4072-d869-dcf439f5a112"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h🔹 جاري إنشاء ملف CSV كبير... قد يستغرق هذا بضع دقائق ⏳\n",
            "✅ تم إنشاء الملف: /content/large_5gb_sample.csv (0.24 GB)\n",
            "\n",
            "=== الطريقة 1: pandas.read_csv(chunksize) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "قراءة الملف بـ Pandas: 100%|██████████| 10/10 [00:02<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 المتوسط (pandas): 49.9987\n",
            "⏱️ الزمن: 2.43 ثانية\n",
            "\n",
            "=== الطريقة 2: Dask DataFrame ===\n",
            " المتوسط (Dask): 49.9987\n",
            " الزمن: 4.30 ثانية\n",
            "\n",
            "=== الطريقة 3: تحويل CSV إلى Parquet ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "تحويل إلى Parquet: 5it [00:04,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ تم إنشاء 5 ملف Parquet\n",
            "📦 إجمالي الحجم: 101.21 MB\n",
            "⏱️ زمن التحويل: 4.56 ثانية\n",
            "\n",
            "=== 📊 مقارنة النتائج ===\n",
            "حجم CSV الأصلي: 0.24 GB\n",
            "Pandas: المتوسط 49.9987 - الزمن 2.43s\n",
            "Dask:   المتوسط 49.9987 - الزمن 4.30s\n",
            "Parquet الحجم: 0.10 GB - زمن التحويل: 4.56s\n",
            "\n",
            "✅ انتهى التنفيذ بنجاح.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}